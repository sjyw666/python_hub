#研发者：时间遗忘
#开发时间：2023/8/13 13:25

'''
岭回归是一种线性回归方法，在一些数据分析的场景下，使用它可以有效地规避过拟合的问题。
岭回归是一种经典的正则化线性回归方法，它的思想是在原有的损失函数（最小二乘法）上加上一个L2惩罚项，用于约束模型参数的大小。该惩罚项的大小由超参数λ控制，λ越大，模型参数越小，越能避免过拟合。
岭回归的损失函数如下所示：
J(w) = (y - Xw)^T(y - Xw) + λ||w||^2
'''

'''
使用方法
在Python中，可以使用sklearn中的Ridge来实现岭回归。Ridge使用了与线性回归类似的接口，可以直接调用fit方法训练模型，调用predict方法预测新数据。
下面是一个简单的示例
'''
from sklearn.linear_model import Ridge
import numpy as np

# 创建训练数据
X_train = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])
y_train = np.array([1, 2, 2, 3])

# 初始化岭回归模型
ridge = Ridge(alpha=1.0)

# 训练模型
ridge.fit(X_train, y_train)

# 预测新数据
X_test = np.array([[3, 3], [3, 4]])
y_predict = ridge.predict(X_test)


'''
超参数调优
在岭回归中，超参数λ的选择对模型的性能影响非常大，如何选择一个合适的λ是非常重要的。
常用的方法有两种：
留出法（hold-out）：将数据集划分成训练集和验证集，分别用于训练和验证模型。
交叉验证法（cross-validation）：将数据集划分成k个互不重叠的子集，每次将其中一个子集作为验证集，其余作为训练集，重复k次，最终将k个结果的平均值作为模型的评价。
下面是使用交叉验证法进行超参数调优的示例
'''
from sklearn.linear_model import RidgeCV

# 创建训练数据
X_train = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])
y_train = np.array([1, 2, 2, 3])

# 初始化岭回归模型
ridgeCV = RidgeCV(alphas=[0.1, 1.0, 10.0], cv=3)

# 训练模型
ridgeCV.fit(X_train, y_train)

# 输出最佳超参数
print('Best alpha:', ridgeCV.alpha_)


'''
岭回归的优点：
能够有效地避免过拟合的问题。
算法简单易懂，计算量小。
支持在线学习（incremental learning），可以方便地应用于大规模数据。

岭回归的缺点：
超参数的选择对模型的性能影响非常大，需要进行调优。
由于使用了L2惩罚项，模型参数会被约束在一个球形的空间里，在某些情况下会导致模型欠拟合。
'''

'''
总结
岭回归是一种经典的线性回归方法，在许多实际应用中都能够取得不错的性能。在实际使用中，需要注意超参数的选择和模型的评估，以及对训练数据的正则化处理。除了岭回归，还有一些其他的正则化线性回归方法，如Lasso和ElasticNet，可以根据具体情况选择合适的方法。
'''
